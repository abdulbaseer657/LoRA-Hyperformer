<h1 align="center">LoRA Hyperformer</h1>

<h2 align="center">EÉ´Êœá´€É´á´„ÉªÉ´É¢ Pá´€Ê€á´€á´á´‡á´›á´‡Ê€-Eêœ°êœ°Éªá´„Éªá´‡É´á´› FÉªÉ´á´‡-Tá´œÉ´ÉªÉ´É¢ á´êœ° Lá´€Ê€É¢á´‡ Lá´€É´É¢á´œá´€É¢á´‡ Má´á´…á´‡ÊŸêœ± á´¡Éªá´›Êœ AÊŸÉªÉ¢É´á´á´‡É´á´› Aá´…á´€á´˜á´›á´‡Ê€êœ± á´€É´á´… Lá´RA</h2>

<!-- <p align="center">
  <a href="" target="_blank">
    <img src="https://github.com/abdulbaseer657/gradstem/blob/abdul-setup/static/gradpath.png" alt="LoRA Hyperformer">
  </a>
</p> -->

### Abstract

Large Language Models (LLMs) have become integral to natural language processing, involving initial broad pretraining on generic data followed by fine-tuning for specific tasks or domains. While advancements in Parameter Efficient Fine-Tuning (PEFT) techniques have made strides in reducing resource demands for LLM fine-tuning, they possess individual constraints. This project addresses the challenges posed by PEFT in the context of transformers architecture for sequence-to-sequence tasks, by integrating two pivotal
techniques: Low-Rank Adaptation (LoRA) for computational efficiency and adaptive layers for task-specific customization. To overcome the limitations of LoRA, we introduce a simple yet effective hyper alignment adapter, that leverages a hypernetwork to generate decoder inputs based on encoder outputs, thereby serving as a crucial bridge to improve alignment between the encoder and the decoder. This fusion strikes a balance between the fine-tuning complexity and task performance, mitigating the individual drawbacks while improving the encoder-decoder alignment. As a result, we achieve more precise and contextually relevant sequence generation. The proposed solution improves the overall efficiency and effectiveness of LLMs in sequence-to-sequence tasks, leading to better alignment and more accurate output generation.

## Author

ğŸ‘¤ **Abdul Baseer Mohammed**

- Github: [@abdulbaseer657](https://github.com/abdulbaseer657)
- LinkedIn: [abdul baseer mohammed](https://www.linkedin.com/in/abdul-baseer-mohammed-59bbbb158/)

## ğŸ¤ Contributing

Contributions, issues and feature requests are welcome!<br />Feel free to check [issues page](https://github.com/abdulbaseer657/lora-hyperformer/issues).

## Show your support

Give a â­ï¸ if this project helped you!

## ğŸ“ License

Copyright Â© 2023 [Abdul Baseer Mohammed](https://github.com/abdulbaseer657).<br />
This project is [MIT](https://github.com/abdulbaseer657/gradstem/blob/main/LICENSE) licensed.
