<h1 align="center">LoRA Hyperformer</h1>

<h2 align="center">Eɴʜᴀɴᴄɪɴɢ Pᴀʀᴀᴍᴇᴛᴇʀ-Eꜰꜰɪᴄɪᴇɴᴛ Fɪɴᴇ-Tᴜɴɪɴɢ ᴏꜰ Lᴀʀɢᴇ Lᴀɴɢᴜᴀɢᴇ Mᴏᴅᴇʟꜱ ᴡɪᴛʜ Aʟɪɢɴᴍᴇɴᴛ Aᴅᴀᴘᴛᴇʀꜱ ᴀɴᴅ LᴏRA</h2>

<!-- <p align="center">
  <a href="" target="_blank">
    <img src="https://github.com/abdulbaseer657/gradstem/blob/abdul-setup/static/gradpath.png" alt="LoRA Hyperformer">
  </a>
</p> -->

### Abstract

Large Language Models (LLMs) have become integral to natural language processing, involving initial broad pretraining on generic data followed by fine-tuning for specific tasks or domains. While advancements in Parameter Efficient Fine-Tuning (PEFT) techniques have made strides in reducing resource demands for LLM fine-tuning, they possess individual constraints. This project addresses the challenges posed by PEFT in the context of transformers architecture for sequence-to-sequence tasks, by integrating two pivotal
techniques: Low-Rank Adaptation (LoRA) for computational efficiency and adaptive layers for task-specific customization. To overcome the limitations of LoRA, we introduce a simple yet effective hyper alignment adapter, that leverages a hypernetwork to generate decoder inputs based on encoder outputs, thereby serving as a crucial bridge to improve alignment between the encoder and the decoder. This fusion strikes a balance between the fine-tuning complexity and task performance, mitigating the individual drawbacks while improving the encoder-decoder alignment. As a result, we achieve more precise and contextually relevant sequence generation. The proposed solution improves the overall efficiency and effectiveness of LLMs in sequence-to-sequence tasks, leading to better alignment and more accurate output generation.

## Author

👤 **Abdul Baseer Mohammed**

- Github: [@abdulbaseer657](https://github.com/abdulbaseer657)
- LinkedIn: [abdul baseer mohammed](https://www.linkedin.com/in/abdul-baseer-mohammed-59bbbb158/)

## 🤝 Contributing

Contributions, issues and feature requests are welcome!<br />Feel free to check [issues page](https://github.com/abdulbaseer657/lora-hyperformer/issues).

## Show your support

Give a ⭐️ if this project helped you!

## 📝 License

Copyright © 2023 [Abdul Baseer Mohammed](https://github.com/abdulbaseer657).<br />
This project is [MIT](https://github.com/abdulbaseer657/gradstem/blob/main/LICENSE) licensed.
